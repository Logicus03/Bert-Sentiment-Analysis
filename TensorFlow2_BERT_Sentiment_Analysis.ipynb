{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TensorFlow2_BERT_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Logicus03/Bert-Sentiment-Analysis-/blob/master/TensorFlow2_BERT_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o2nRY4GyKWqL"
      },
      "source": [
        "# TensorFlow 2 - BERT: Tweet Sentiment Analysis\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) - A pre-trained BERT model can be fine tuned to create state-of-the-art models for a wide range of NLP tasks such as question answering, sentiment analysis and named entity recognition.\n",
        "\n",
        "**Dataset**\n",
        "\n",
        "Tweet dataset has tweets dataset for natural language processing.\n",
        "Please download the dataset from [Kaggle link](www.kaggle.com/dataset/4af304c0f797e3b08f22895d6a0dcf95eee4c37f7a20775c7a4ee2281c6ba2ff).\n",
        "\n",
        "**Problem**\n",
        "\n",
        "A text in tweets dataset is either positive or negative. Therefore, the NLP tweet sentiment analysis task is a supervised learning binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6OrkTAeiI77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "761956f5-059b-4616-a92b-6c042350e749"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bvh3ECwPcRI6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "67d18288-a84d-4cde-c45a-741efe851117"
      },
      "source": [
        "# Install the required package\n",
        "!pip install bert-for-tf2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 1.8MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=fb3fd931fd65078d83dafad1191d24f9cc7acdbee0298ba23c0993679999a14e\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=992812e07ff148b8d90ceb1d6c69e81777a6fff72d95036a9a0e440f31aa1c31\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp36-none-any.whl size=19473 sha256=b7a0731d2825ad8d998657e7dbb34a5e4c3918a27aa83ef3d472b4dcc89e73ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.2 py-params-0.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dotZjUvecRI8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a7869b66-c61a-439e-86b3-110bd10f4b0f"
      },
      "source": [
        "# Import modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import bert\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import  Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"TensorFlow Version:\",tf.__version__)\n",
        "print(\"Hub version: \",hub.__version__)\n",
        "pd.set_option('display.max_colwidth',1000)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.2.0\n",
            "Hub version:  0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-uH1zw2Pa13m"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mWY9gKxbDk-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "085dba84-8e66-4b58-b610-1e3765ad6df7"
      },
      "source": [
        "# Montar o Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W379xRxlMbtb",
        "colab": {}
      },
      "source": [
        "# Read the IMDB Dataset.csv into Pandas dataframe\n",
        "df=pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/0. Mestrado/train3n.csv\", error_bad_lines=False, sep=';')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmE5-_RebmIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_treated_data(dataset, cols, cols_drop = [], col_to_change='sentiment', val_col_change = {\"Negativo\": 0, \"Positivo\":1}):\n",
        "    \n",
        "    # # 1. Criar a variável \"data\"\n",
        "    # dataset = pd.read_csv( \n",
        "    #     DATASET_PATH,\n",
        "    #     engine=\"python\", \n",
        "    #     encoding=\"latin1\"\n",
        "    # )\n",
        "        \n",
        "    # 2. Rename columns\n",
        "    dataset.columns = cols\n",
        "    \n",
        "    # 3. Drop columns not needed\n",
        "    dataset.drop(cols_drop, axis=1, inplace=True)\n",
        "    \n",
        "    # 3.1 Drop all rows with at least one element is missing\n",
        "    dataset.dropna()\n",
        "    \n",
        "    # 4. Convert setiments from \"Negative/Positive\" to \"0/1\" \n",
        "    # dataset.replace({col_to_change: val_col_change}, inplace=True)\n",
        "    \n",
        "    # Return our dataset\n",
        "    return dataset"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTuXagdDtq1v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "db559903-7865-44fe-b338-3e6329440d7b"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>tweet_date</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>query_used</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1050785521201541121</td>\n",
              "      <td>@Laranjito76 A pessoa certa para isso seria o vale e azevedo :)</td>\n",
              "      <td>Fri Oct 12 16:29:25 +0000 2018</td>\n",
              "      <td>1</td>\n",
              "      <td>:)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1050785431955140608</td>\n",
              "      <td>@behin_d_curtain Para mim, é precisamente o contrário :) Vem a chuva e vem a boa disposição :)</td>\n",
              "      <td>Fri Oct 12 16:29:04 +0000 2018</td>\n",
              "      <td>1</td>\n",
              "      <td>:)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1050785401248645120</td>\n",
              "      <td>Vou fazer um video hoje... estou pensando em falar um pouco sobre o novo meta do CSGO e sobre a pagina https://t.co/5RjhKnj0oh Alguem tem uma sugestao? Queria falar sobre algo do cenario nacional :D</td>\n",
              "      <td>Fri Oct 12 16:28:56 +0000 2018</td>\n",
              "      <td>1</td>\n",
              "      <td>:)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1050785370982547461</td>\n",
              "      <td>aaaaaaaa amei tanto essas polaroids, nem sei expressar o quanto eu to apaixonada de vdd✨💖🎈🎉🎊 espero que outras pessoas consigam ganhar também :) https://t.co/pbIp7tRcSE</td>\n",
              "      <td>Fri Oct 12 16:28:49 +0000 2018</td>\n",
              "      <td>1</td>\n",
              "      <td>:)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1050785368902131713</td>\n",
              "      <td>Valoriza o coração do menininho que vc tem. Ele é diferente. O faça sorrir e ter certeza disso ❤️ — Eu valorizo todo mundo na minha vida, não vai ser diferente com ele :)) https://t.co/5c7wlXQyz9</td>\n",
              "      <td>Fri Oct 12 16:28:49 +0000 2018</td>\n",
              "      <td>1</td>\n",
              "      <td>:)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    id  ... query_used\n",
              "0  1050785521201541121  ...         :)\n",
              "1  1050785431955140608  ...         :)\n",
              "2  1050785401248645120  ...         :)\n",
              "3  1050785370982547461  ...         :)\n",
              "4  1050785368902131713  ...         :)\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MTnRZMubw6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "default_cols = [\"id\", \"text\", \"date\", \"sentiment\", \"query\"];\n",
        "default_drop_cols = [\"id\", \"date\", \"query\"]\n",
        "# default_cols = [\"sentiment\", \"text\"];\n",
        "# default_drop_cols = [\"id\", \"date\", \"query\"]\n",
        "\n",
        "# class_names = ['Negativo', 'Positive']\n",
        "df = get_treated_data(df, default_cols, cols_drop = default_drop_cols)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5GbNMjbS_b8J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "121fb657-1e3d-4095-c356-d209c263b4a4"
      },
      "source": [
        "# Take a peek at the dataset\n",
        "df[\"sentiment\"].value_counts(normalize=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.33334\n",
              "2    0.33333\n",
              "0    0.33333\n",
              "Name: sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTsrFxRWcOE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text):\n",
        "    \n",
        "    # Not needed to be imported globally\n",
        "    from bs4 import BeautifulSoup\n",
        "    import re\n",
        "    text = BeautifulSoup(text, \"lxml\").get_text()\n",
        "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE) # Remove urls\n",
        "    text = re.sub(r\"@[A-Za-z0-9]+\", ' ', text)\n",
        "    text = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', text)\n",
        "    text = re.sub(r\"[^a-zA-Z.!?']\", ' ', text)\n",
        "    text = re.sub(r\" +\", ' ', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmQwvcPEckhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f2862bd-1818-4fe3-867a-69e66bd84d4d"
      },
      "source": [
        "df['text'] = df['text'].apply(lambda text: preprocess_text(text))\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/V3MvuBeA6V\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/g7PjQO2lvv\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/k7CeF441je\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/CflxDyKckX\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/meA9NVQ8Xv\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/SqfTv2jFkB\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/T0MCRdQurV\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/I58krHgIop\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/I58krGZ7wR\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/20laNmO234\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/Fr5HBYnf9P\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/iHSV24IOgV\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/egFHcD9eKR\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/v4qULxZRAK\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/zFfX7pqPXB\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/340CqdGSkR\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/BIEzXPOORU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/340Cqdphtj\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/JhbniDn5wj\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/zFfX7pIrmb\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/fdKkCKJWoo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/GEE3XMOGOK\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/LigSPE4XE5\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/20laNmwqEu\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/c39o7GiAIy\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/4B6zjeOCef\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/PO3QFXYRTg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/kjWWU5qYPa\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/hCvatxf4Ve\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/HdeQ88LpuK\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/K8stIRoLMU\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/mVN5Wror7K\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/JsIsfGCXp2\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/pZ0ffAybsB\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/i8vunCwigv\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/pG6TwgsbSg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/hIXfumECmV\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/i8vunCNTF5\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/asj9w2ZXW9\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/9YIfDaEPJL\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/HdDZ5scGIv\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/12zmhGd4a8\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/zWFbgqCPHE\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"https://t.co/wKuGiUQ8G4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A pessoa certa para isso seria o vale e azevedo</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d curtain Para mim precisamente o contr rio Vem a chuva e vem a boa disposi o</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Vou fazer um video hoje... estou pensando em falar um pouco sobre o novo meta do CSGO e sobre a pagina Alguem tem uma sugestao? Queria falar sobre algo do cenario nacional D</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aaaaaaaa amei tanto essas polaroids nem sei expressar o quanto eu to apaixonada de vdd espero que outras pessoas consigam ganhar tamb m</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Valoriza o cora o do menininho que vc tem. Ele diferente. O fa a sorrir e ter certeza disso Eu valorizo todo mundo na minha vida n o vai ser diferente com ele</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                            text  sentiment\n",
              "0                                                                                                                               A pessoa certa para isso seria o vale e azevedo           1\n",
              "1                                                                                                 d curtain Para mim precisamente o contr rio Vem a chuva e vem a boa disposi o           1\n",
              "2  Vou fazer um video hoje... estou pensando em falar um pouco sobre o novo meta do CSGO e sobre a pagina Alguem tem uma sugestao? Queria falar sobre algo do cenario nacional D          1\n",
              "3                                       aaaaaaaa amei tanto essas polaroids nem sei expressar o quanto eu to apaixonada de vdd espero que outras pessoas consigam ganhar tamb m           1\n",
              "4                Valoriza o cora o do menininho que vc tem. Ele diferente. O fa a sorrir e ter certeza disso Eu valorizo todo mundo na minha vida n o vai ser diferente com ele           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gbcp65zz_7wk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5eba1187-277d-44b8-9c5c-2951b20bb596"
      },
      "source": [
        "print(\"The number of rows and columns in the dataset is: {}\".format(df.shape))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of rows and columns in the dataset is: (100000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h_uP_hOAR_ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e19ced59-0b1b-4431-f4e8-49ae6600c538"
      },
      "source": [
        "# Identify missing values\n",
        "df.apply(lambda x: sum(x.isnull()), axis=0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         0\n",
              "sentiment    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AelQrBAeAEuJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "53f9ff69-ddee-4ea2-e0cc-4b581d808ce4"
      },
      "source": [
        "# Check the target class balance\n",
        "df[\"sentiment\"].value_counts(normalize=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    0.33334\n",
              "2    0.33333\n",
              "0    0.33333\n",
              "Name: sentiment, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE7HnvcfXiUY",
        "colab_type": "text"
      },
      "source": [
        "**Download token**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs4wDf4QOwWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13fdeba6-421d-4652-b266-0f26bf588104"
      },
      "source": [
        "!rm -rf bert-base-portuguese-cased\n",
        "!mkdir bert-base-portuguese-cased\n",
        "!wget https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
        "!wget https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt \n",
        "\n",
        "!apt-get install unzip\n",
        "\n",
        "!unzip bert-base-portuguese-cased_pytorch_checkpoint.zip -d bert-base-portuguese-cased\n",
        "!mv vocab.txt bert-base-portuguese-cased/vocab.txt \n",
        "!pip install -U transformers"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-12 11:38:41--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
            "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.96.8\n",
            "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.96.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 406220891 (387M) [application/zip]\n",
            "Saving to: ‘bert-base-portuguese-cased_pytorch_checkpoint.zip’\n",
            "\n",
            "\r          bert-base   0%[                    ]       0  --.-KB/s               \r         bert-base-   1%[                    ]   5.40M  27.0MB/s               \r        bert-base-p   5%[>                   ]  22.55M  55.0MB/s               \r       bert-base-po  10%[=>                  ]  39.62M  64.9MB/s               \r      bert-base-por  14%[=>                  ]  56.88M  70.2MB/s               \r     bert-base-port  19%[==>                 ]  73.80M  73.0MB/s               \r    bert-base-portu  23%[===>                ]  90.94M  75.1MB/s               \r   bert-base-portug  28%[====>               ] 109.02M  76.5MB/s               \r  bert-base-portugu  32%[=====>              ] 126.05M  77.6MB/s               \r bert-base-portugue  37%[======>             ] 144.18M  79.0MB/s               \rbert-base-portugues  41%[=======>            ] 161.72M  79.9MB/s               \rert-base-portuguese  46%[========>           ] 178.76M  80.2MB/s               \rrt-base-portuguese-  50%[=========>          ] 196.42M  80.9MB/s               \rt-base-portuguese-c  55%[==========>         ] 213.79M  81.0MB/s               \r-base-portuguese-ca  59%[==========>         ] 230.95M  81.3MB/s               \rbase-portuguese-cas  64%[===========>        ] 248.60M  81.8MB/s    eta 2s     \rase-portuguese-case  68%[============>       ] 265.99M  81.8MB/s    eta 2s     \rse-portuguese-cased  73%[=============>      ] 284.29M  85.8MB/s    eta 2s     \re-portuguese-cased_  77%[==============>     ] 302.02M  86.0MB/s    eta 2s     \r-portuguese-cased_p  82%[===============>    ] 320.48M  86.0MB/s    eta 2s     \rportuguese-cased_py  87%[================>   ] 338.10M  85.9MB/s    eta 1s     \rortuguese-cased_pyt  92%[=================>  ] 356.80M  86.9MB/s    eta 1s     \rrtuguese-cased_pyto  96%[==================> ] 374.19M  86.3MB/s    eta 1s     \rbert-base-portugues 100%[===================>] 387.40M  86.7MB/s    in 4.6s    \n",
            "\n",
            "2020-07-12 11:38:46 (83.6 MB/s) - ‘bert-base-portuguese-cased_pytorch_checkpoint.zip’ saved [406220891/406220891]\n",
            "\n",
            "--2020-07-12 11:38:56--  https://neuralmind-ai.s3.us-east-2.amazonaws.com/nlp/bert-base-portuguese-cased/vocab.txt\n",
            "Resolving neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)... 52.219.105.82\n",
            "Connecting to neuralmind-ai.s3.us-east-2.amazonaws.com (neuralmind-ai.s3.us-east-2.amazonaws.com)|52.219.105.82|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209528 (205K) [text/plain]\n",
            "Saving to: ‘vocab.txt’\n",
            "\n",
            "vocab.txt           100%[===================>] 204.62K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-07-12 11:38:56 (2.12 MB/s) - ‘vocab.txt’ saved [209528/209528]\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-21ubuntu1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n",
            "Archive:  bert-base-portuguese-cased_pytorch_checkpoint.zip\n",
            "  inflating: bert-base-portuguese-cased/config.json  \n",
            "  inflating: bert-base-portuguese-cased/pytorch_model.bin  \n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=e4414df975adf99260dc6601d4984a8b73ffbe4456e8f6d0d445216f3167eb7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAQ1jHpmXoId",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "91f25992-b18c-43b4-eeb5-e21788d79207"
      },
      "source": [
        "from transformers import BertTokenizer, BertConfig, TFBertModel\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-portuguese-cased\", from_pt=True)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertModel.\n",
            "\n",
            "Some weights or buffers of the PyTorch model TFBertModel were not initialized from the TF 2.0 model and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ESyS2xsQbQlm",
        "colab": {}
      },
      "source": [
        "# Functions for constructing BERT Embeddings: input_ids, input_masks, input_segments and Inputs\n",
        "MAX_SEQ_LEN=500 # max sequence length\n",
        "\n",
        "def get_masks(tokens):\n",
        "    \"\"\"Masks: 1 for real tokens and 0 for paddings\"\"\"\n",
        "    return [1]*len(tokens) + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        " \n",
        "def get_segments(tokens):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"  \n",
        "    segments = []\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = 1\n",
        "    return segments + [0] * (MAX_SEQ_LEN - len(tokens))\n",
        "\n",
        "def get_ids(tokens, tokenizer):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
        "    input_ids = token_ids + [0] * (MAX_SEQ_LEN - len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def create_single_input(sentence, tokenizer, max_len):\n",
        "    \"\"\"Create an input from a sentence\"\"\"\n",
        "    stokens = tokenizer.tokenize(sentence)\n",
        "    stokens = stokens[:max_len]\n",
        "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        " \n",
        "    ids = get_ids(stokens, tokenizer)\n",
        "    masks = get_masks(stokens)\n",
        "    segments = get_segments(stokens)\n",
        "\n",
        "    return ids, masks, segments\n",
        " \n",
        "def convert_sentences_to_features(sentences, tokenizer):\n",
        "    \"\"\"Convert sentences to features: input_ids, input_masks and input_segments\"\"\"\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        " \n",
        "    for sentence in tqdm(sentences,position=0, leave=True):\n",
        "      ids,masks,segments=create_single_input(sentence,tokenizer,MAX_SEQ_LEN-2)\n",
        "      assert len(ids) == MAX_SEQ_LEN\n",
        "      assert len(masks) == MAX_SEQ_LEN\n",
        "      assert len(segments) == MAX_SEQ_LEN\n",
        "      input_ids.append(ids)\n",
        "      input_masks.append(masks)\n",
        "      input_segments.append(segments)\n",
        "\n",
        "    return [np.asarray(input_ids, dtype=np.int32), \n",
        "          np.asarray(input_masks, dtype=np.int32), \n",
        "          np.asarray(input_segments, dtype=np.int32)]\n",
        "\n",
        "def create_tonkenizer(bert_layer):\n",
        "    \"\"\"Instantiate Tokenizer with vocab\"\"\"\n",
        "    # vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    # do_lower_case=bert_layer.resolved_object.do_lower_case.numpy() \n",
        "    # tokenizer=bert.bert_tokenization.FullTokenizer(vocab_file,do_lower_case)\n",
        "    do_lower_case = False\n",
        "    tokenizer = BertTokenizer(\"bert-base-portuguese-cased/vocab.txt\", do_lower_case)\n",
        "    return tokenizer"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xs6_p8VTgpoy"
      },
      "source": [
        "## Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C_mFZ2HRcRJB",
        "colab": {}
      },
      "source": [
        "def nlp_model(callable_object):\n",
        "    # Load the pre-trained BERT base model\n",
        "    # bert_layer = hub.KerasLayer(handle=callable_object, trainable=True)  \n",
        "\n",
        "    bert_layer = callable_object\n",
        "   \n",
        "    # BERT layer three inputs: ids, masks and segments\n",
        "    input_ids = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_ids\")           \n",
        "    input_masks = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"input_masks\")       \n",
        "    input_segments = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    \n",
        "    inputs = [input_ids, input_masks, input_segments] # BERT inputs\n",
        "    # If using hub.KerasLayer, PLEASE, CHANGE THE ORDER of the variables, I mean: \n",
        "    # pooled_output, sequence_output = \n",
        "    sequence_output, pooled_output = bert_layer(inputs) # BERT outputs \n",
        "    \n",
        "    # Add a hidden layer\n",
        "    x = Dense(units=768, activation='relu')(pooled_output)\n",
        "    x = Dropout(0.3)(x)\n",
        " \n",
        "    # Add output layer\n",
        "    outputs = Dense(3, activation=\"softmax\")(x)\n",
        "\n",
        "    # Construct a new model\n",
        "    model = Model(inputs=inputs, outputs=outputs, )\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPQ-WWSidREI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "954d23fd-6b6b-4246-df25-b12bb0ae80a4"
      },
      "source": [
        "model = nlp_model(bert_model)\n",
        "model.summary()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_ids (InputLayer)          [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_masks (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model (TFBertModel)     ((None, 500, 768), ( 108923136   input_ids[0][0]                  \n",
            "                                                                 input_masks[0][0]                \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 768)          590592      tf_bert_model[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 768)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 3)            2307        dropout_37[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 109,516,035\n",
            "Trainable params: 109,516,035\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ViqJURL5qcZJ"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XL5LaS68Qo8G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6463c6df-d1be-4394-8d0e-dc817e649e64"
      },
      "source": [
        "# Create examples for training and testing\n",
        "\n",
        "df = df.sample(frac=1) # Shuffle the dataset\n",
        "tokenizer = create_tonkenizer(model.layers[3])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['sentiment'], \n",
        "    test_size=0.3, \n",
        "    stratify=df['sentiment'], \n",
        "    random_state=15 \n",
        "    )\n",
        "\n",
        "print( \"\\nx_train: {}; \\tX_test: {}\".format(X_train.shape, X_test.shape))\n",
        "print(\"\\ny_test: \\n{}, \\n\\ny_train: \\n{}\".format(y_train.value_counts(normalize=True), y_test.value_counts(normalize=True) ) )\n",
        "\n",
        "X_train = convert_sentences_to_features(X_train, tokenizer)\n",
        "X_test = convert_sentences_to_features(X_test, tokenizer)\n",
        "\n",
        "y_train = to_categorical( y_train )\n",
        "y_test =  to_categorical( y_test )\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/70000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "x_train: (70000,); \tX_test: (30000,)\n",
            "\n",
            "y_test: \n",
            "1    0.333343\n",
            "2    0.333329\n",
            "0    0.333329\n",
            "Name: sentiment, dtype: float64, \n",
            "\n",
            "y_train: \n",
            "2    0.333333\n",
            "1    0.333333\n",
            "0    0.333333\n",
            "Name: sentiment, dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 70000/70000 [00:14<00:00, 4969.09it/s]\n",
            "100%|██████████| 30000/30000 [00:06<00:00, 4993.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8CAhbzjnk8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7a6c5c66-9609-43fe-b2cc-af6ee1fe6066"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYv9EW9O0rHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# callback\n",
        "\n",
        "checkpoint_path = \"./sentiment_analysis_model\"\n",
        "ckpt = tf.train.Checkpoint(model=model)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "class CustomCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        ckpt_manager.save()\n",
        "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IzSF2HJi5XsY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "583cfb79-0c49-4f3a-c130-a58cd5118298"
      },
      "source": [
        "# Train the model\n",
        "BATCH_SIZE = 10\n",
        "EPOCHS = 2\n",
        "\n",
        "# Use Adam optimizer to minimize the categorical_crossentropy loss\n",
        "opt = Adam(learning_rate=2e-5)\n",
        "\n",
        "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "\n",
        "# softmax_cross_entropy_with_logits\n",
        "model.compile(optimizer=opt, \n",
        "              loss= 'categorical_crossentropy', #binary_crossentropy\n",
        "              metrics = ['categorical_accuracy']\n",
        "              )\n",
        "\n",
        "# Fit the data to the model\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    verbose = 1,\n",
        "                    callbacks=[CustomCallback()]\n",
        "                    )\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "7000/7000 [==============================] - ETA: 0s - loss: 0.3416 - categorical_accuracy: 0.8449Checkpoint saved at ./sentiment_analysis_model.\n",
            "7000/7000 [==============================] - 5398s 771ms/step - loss: 0.3416 - categorical_accuracy: 0.8449 - val_loss: 0.2866 - val_categorical_accuracy: 0.8707\n",
            "Epoch 2/2\n",
            "7000/7000 [==============================] - ETA: 0s - loss: 0.2433 - categorical_accuracy: 0.8968Checkpoint saved at ./sentiment_analysis_model.\n",
            "7000/7000 [==============================] - 5394s 771ms/step - loss: 0.2433 - categorical_accuracy: 0.8968 - val_loss: 0.3480 - val_categorical_accuracy: 0.8638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zmw_IyL1172",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, name, path, h5=False):\n",
        "  '''\n",
        "  model, model_name, path, h5(optional)\n",
        "  '''\n",
        "  if h5:\n",
        "    !pip install -q pyyaml h5py  # Required to save models in HDF5 format\n",
        "    model.save( \"{}.h5\".format(name) )\n",
        "  else:\n",
        "    model.save( name )\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4tg3sFSMRv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "05f2e467-4937-418f-abf8-aa856c9cd966"
      },
      "source": [
        "save_model(model, \"sentiment_model\", \"trained_model\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: sentiment_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmwK7C4-re9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d7f58567-17a1-4063-857d-0295aebab217"
      },
      "source": [
        "history.history"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'categorical_accuracy': [0.8448571562767029, 0.8967857360839844],\n",
              " 'loss': [0.3415980637073517, 0.2433004528284073],\n",
              " 'val_categorical_accuracy': [0.8707333207130432, 0.8638333082199097],\n",
              " 'val_loss': [0.28664323687553406, 0.3480300009250641]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "toHq_3_Am6if"
      },
      "source": [
        "## Analysis of model performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fcuvU9EhiFXX",
        "colab": {}
      },
      "source": [
        "# # Load the pretrained nlp_model\n",
        "# from tensorflow.keras.models import load_model\n",
        "# new_model = load_model('test')\n",
        "# new_model.summary\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KCqF3D54iGiZ",
        "colab": {}
      },
      "source": [
        "# Predict on test dataset\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "pred_test = np.argmax(model.predict(X_test), axis=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6pcfuN3wvFJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "8bb0c103-cccc-47b7-99bc-5d6b6c8a11b7"
      },
      "source": [
        "print(classification_report(np.argmax(y_test,axis=1), pred_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.89      0.82     10000\n",
            "           1       0.87      0.71      0.78     10000\n",
            "           2       0.96      1.00      0.98     10000\n",
            "\n",
            "    accuracy                           0.86     30000\n",
            "   macro avg       0.87      0.86      0.86     30000\n",
            "weighted avg       0.87      0.86      0.86     30000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZxyAqz094Lzv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "74ae77a2-9e3a-4b17-b77a-433d6fb658e7"
      },
      "source": [
        "print(pred_test[:40])\n",
        "print( y_test[:40].argmax(1) )"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 0 0 1 2 2 0 2 1 1 2 2 2 1 2 2 2 2 0 1 0 0 1 1 2 2 1 0 1 0 0 1 2 0 2 0\n",
            " 0 0 1]\n",
            "[1 2 0 0 1 2 2 0 2 0 1 2 2 2 1 2 2 2 2 0 0 1 0 1 1 2 2 1 0 1 1 0 1 2 0 2 0\n",
            " 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA653kqqM5bk",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-tvup1vj-mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(model_, sentence):\n",
        "  sent = []\n",
        "  sent.append(sentence)\n",
        "  sentence_feature = convert_sentences_to_features(sent, tokenizer)\n",
        "\n",
        "  prediction = np.argmax(model_.predict( sentence_feature ) , axis=1) \n",
        "\n",
        "  # Show Positivo/Negativo\n",
        "  pred = [\"Negativo\" if x == 0 else \"Positivo\" if x == 2 else \"Neutro\"  for x in prediction]\n",
        "\n",
        "  return pred"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR4x04shSszC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "818812de-04a6-4c20-9c74-3df71ad4bb36"
      },
      "source": [
        "# Predict\n",
        "get_predictions( model, \"Aquele ator é ruim\" )"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1994.44it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Negativo']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaVD-Upun0Pd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0dec13bc-e38c-4c26-aa07-ae7cefa2b229"
      },
      "source": [
        "get_predictions( model, \"Eu gosto do seu sorriso\" )"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1916.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Neutro']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}